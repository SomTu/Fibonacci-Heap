{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100 - Final project.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomTu/Fibonacci-Heap/blob/main/100_Final_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ZZfC2Rl9f4"
      },
      "source": [
        "# The Goal\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKy9U3s_rzSv"
      },
      "source": [
        "The goal changed twice throughout the analysis. The dataset consits of about 3000 reviews of flights done by British Airlines. First I was trying to test the hypothesis that people who travel a lot would leave more positive reviews than those who travel less. That showed to be false.\n",
        "\n",
        "Then, I thought about whether the reviews' bodies (texts written by users) could somehow predict their Overall Rating. That should be possible, however, I wasn't able to get any meaningful result.\n",
        "\n",
        "The last approach succeeded as I tried to just predict the Overall Rating of customers from their ratings of different services during and before the flight. This information can tell us, which services are \"the most important\".\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNAqrM59Sp0S"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1XwYQA7ag9m"
      },
      "source": [
        "The original dataset looked like this:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ",OverallRating,ReviewHeader,Name,Datetime,VerifiedReview,ReviewBody,TypeOfTraveller,SeatType,Route,DateFlown,SeatComfort,CabinStaffService,GroundService,ValueForMoney,Recommended,Aircraft,Food&Beverages,InflightEntertainment,Wifi&Connectivity\n",
        "\n",
        "0,1,\"\"\"Service level far worse then Ryanair\"\"\",L Keele,19th November 2023,True,\"4 Hours before takeoff we received a Mail stating a cryptic message that there are disruptions to be expected as there is a limit on how many planes can leave at the same time. So did the capacity of the Heathrow Airport really hit British Airways by surprise, 4h before departure? Anyhow - we took the one hour delay so what - but then we have been forced to check in our Hand luggage. I travel only with hand luggage to avoid waiting for the ultra slow processing of the checked in luggage. Overall 2h later at home than planed, with really no reason, just due to incompetent people. Service level far worse then Ryanair and triple the price. Really never again. Thanks for nothing.\",Couple Leisure,Economy Class,London to Stuttgart,November 2023,1,1,1,1,no,,,,"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gzSHyWPJQowB",
        "outputId": "0f75a73d-e067-47cf-b7f9-aa8feefd047b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid decimal literal (<ipython-input-4-7381b36a5d6e>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-7381b36a5d6e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    0,1,\"\"\"Service level far worse then Ryanair\"\"\",L Keele,19th November 2023,True,\"4 Hours before takeoff we received a Mail stating a cryptic message that there are disruptions to be expected as there is a limit on how many planes can leave at the same time. So did the capacity of the Heathrow Airport really hit British Airways by surprise, 4h before departure? Anyhow - we took the one hour delay so what - but then we have been forced to check in our Hand luggage. I travel only with hand luggage to avoid waiting for the ultra slow processing of the checked in luggage. Overall 2h later at home than planed, with really no reason, just due to incompetent people. Service level far worse then Ryanair and triple the price. Really never again. Thanks for nothing.\",Couple Leisure,Economy Class,London to Stuttgart,November 2023,1,1,1,1,no,,,,\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From all of these data I chose only the OverallRating and the features saved as interer values (like ValueForMoney for example). This lead to a cropped dataset, where I added the value \"0\" wherever the value was nonexistent. Dataset then looked like this:\n"
      ],
      "metadata": {
        "id": "e-KmBTtbQ2P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OverallRating,SeatComfort,CabinStaffService,GroundService,ValueForMoney,Food&Beverages,InflightEntertainment,Wifi&Connectivity\n",
        "1,1,1,1,1,0,0,0\n",
        "3,2,3,1,2,1,2,2\n",
        "8,3,3,4,3,4,0,0\n",
        "1,3,3,1,1,0,0,0"
      ],
      "metadata": {
        "id": "p2i-RtdwRNR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze the data, I chose to use PCA to determine which features could be the best to optimize if one intends to have better rating (and therefore probably customer experience). I also did a linear regression to make a model predicting overall rating from studied features. That could serve for experiments on land (where could airline test individual services and from their results determine, whether it made any difference)"
      ],
      "metadata": {
        "id": "nGG7OILWSB3u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlPp5EXLirb6"
      },
      "source": [
        "\n",
        "dataset: https://drive.google.com/file/d/1lxhDmSw6tw7HXIArJlcnYl73WHwNeWSF/view?usp=drive_link\n",
        "\n",
        "original link: https://www.kaggle.com/datasets/chaudharyanshul/airline-reviews/data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sjuM3x-aGcln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sG6-apMCtlC"
      },
      "source": [
        "from csv import reader as _reader\n",
        "from sklearn.model_selection import train_test_split as TTS\n",
        "from random import randint\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "filename = \"https://drive.google.com/file/d/1lxhDmSw6tw7HXIArJlcnYl73WHwNeWSF/view?usp=drive_link/cropped_data.csv\"\n",
        "file = open(filename, mode=\"rt\", encoding=\"utf-8\")\n",
        "reader = _reader(file, delimiter=\",\")\n",
        "\n",
        "# Prepare data\n",
        "X, y = datasets.make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=123)\n",
        "\n",
        "# more preference of one class\n",
        "y[:int(3*len(y)/4)] = 1\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owqzIA6Mi2wA"
      },
      "source": [
        "### Your metod in $\\text{class}$ or $\\text{def}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTMv6Fk0zX-f"
      },
      "source": [
        "Here describe why you choose this classifier - if this classifier differs from what we had in our assignments, so explain its functionality.\n",
        "\n",
        "$$ \\hat{y_i} = \\text{rand}_{weighted}(X) \\;\\;\\;\\;\\;\\; \\text{where}\\;\\; weighted \\approx \\{P(y_1), P(y_1), ..., P(y_n)\\}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qI9C2PhjDJP"
      },
      "source": [
        "class MyDummyClassifier:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    n_samples, n_features = X.shape\n",
        "    self._classes = np.unique(y)\n",
        "    self._priors = [X[c==y].shape[0] / float(n_samples) for c in self._classes]\n",
        "\n",
        "  def predict(self, X):\n",
        "    print(self._priors)\n",
        "    return np.random.choice(self._classes, X.shape[0], p=self._priors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL-3MkCTjGbW"
      },
      "source": [
        "### Train your classifier(s) and predict the testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxaVVzZCjTQ5",
        "outputId": "6ee7061c-862e-4a90-ba40-8dbb3df2dd6c"
      },
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "  return accuracy\n",
        "\n",
        "# Design a model\n",
        "clf = MyDummyClassifier()\n",
        "\n",
        "# 4. Train a model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# 6. Evaluate predictions\n",
        "print(\"Dummy classifier has accuracy\", accuracy(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.12625, 0.87375]\n",
            "Dummy classifier has accuracy 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFA8079oj8xY"
      },
      "source": [
        "## Discuss the results\n",
        "\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
        "\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import reader as _reader\n",
        "#from csv import writer as _writer\n",
        "#from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split as TTS\n",
        "from random import randint\n",
        "#from nltk.tokenize import word_tokenize\n",
        "#from nltk import download as DWNLD\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "#from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "#DWNLD('punkt_tab')##################\n",
        "\n",
        "filename = \"cropped_data.csv\"\n",
        "file = open(filename, mode=\"rt\", encoding=\"utf-8\")\n",
        "reader = _reader(file, delimiter=\",\")\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "skipped_first_row = False\n",
        "for row in reader:\n",
        "\tif not skipped_first_row:\n",
        "\t\tskipped_first_row = True\n",
        "\t\tcontinue\n",
        "\ty.append(int(row[0]))\n",
        "\tX.append([int(e) for e in row[1:]])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = TTS(X, y, test_size=0.3, random_state=randint(0, 10000))\n",
        "\n",
        "class PCA:\n",
        "\tdef __init__(self, threshold):\n",
        "\t\tself.threshold = threshold\n",
        "\t\tself.n_components = None\n",
        "\t\tself.components = None\n",
        "\t\tself.mean = None\n",
        "\t\tself.idxs = None\n",
        "\t\tself.eigenvalues = None\n",
        "\n",
        "\tdef fit(self, X):\n",
        "\t\t# mean\n",
        "\t\tself.mean = np.mean(X, axis=0)\n",
        "\t\tX = X - self.mean\n",
        "\t\t# covariance\n",
        "\t\t# row  = 1 sample, columns = feature >> we have to transform because of np.cov()\n",
        "\t\tcov = np.cov(X.T)\n",
        "\t\t# eigenvectors and eigenvalues\n",
        "\t\tself.eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
        "\t\t#  v[:, i]\n",
        "\t\t# sort eigenvectors\n",
        "\t\teigenvectors = eigenvectors.T\n",
        "\t\tself.idxs = np.argsort(self.eigenvalues)[::-1]\n",
        "\t\tself.eigenvalues = self.eigenvalues[self.idxs]\n",
        "\t\teigenvectors = eigenvectors[self.idxs]\n",
        "\t\t#store first n eigenvectors\n",
        "\t\tself.n_components = self._find_number_of_components(self.threshold)\n",
        "\t\tself.components = eigenvectors[0:self.n_components]\n",
        "\n",
        "\tdef transform(self, X):\n",
        "\t\tX = X - self.mean\n",
        "\t\treturn np.dot(X, self.components.T)\n",
        "\n",
        "\tdef _find_number_of_components(self, desired_coverage_of_variance):\n",
        "\t\tif desired_coverage_of_variance < 0 or desired_coverage_of_variance > 1:\n",
        "\t\t\traise Exception(\"desired_coverage_of_variance needs to be in the interval [0, 1]\")\n",
        "\t\t_sum = np.sum(self.eigenvalues)\n",
        "\t\teig = self.eigenvalues / _sum # normalized eigenvalues\n",
        "\t\tpartial_sum = 0\n",
        "\t\tfor index, e in enumerate(eig):\n",
        "\t\t\tpartial_sum += e\n",
        "\t\t\tif partial_sum >= desired_coverage_of_variance:\n",
        "\t\t\t\treturn index + 1 ## we want the number of, not the index of eigenvalues\n",
        "\t\treturn len(self.eigenvalues) + 1\n",
        "\n",
        "pca = PCA(0.2)\n",
        "#print(X_train)\n",
        "\n",
        "\n",
        "#exit()\n",
        "\n",
        "\n",
        "pca.fit(X_train)\n",
        "print(pca.idxs[0:pca.n_components])\n",
        "print(pca.idxs)\n",
        "\n",
        "def transform_data(arr, index_set):\n",
        "\tnew_list = []\n",
        "\tfor e in arr:\n",
        "\t\te = e[index_set]\n",
        "\t\tnew_list.append(e)\n",
        "\treturn np.array(new_list)\n",
        "X_train = pca.transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "#X_train = transform_data(X_train, indexes)\n",
        "#X_test = transform_data(X_test, indexes)\n",
        "\n",
        "print(X_train.shape, X_test.shape)\n",
        "\n",
        "class OurLinearRegression:\n",
        "\tdef __init__(self, learning_rate=0.001, n_iters=1000):\n",
        "\t\tself.lr = learning_rate\n",
        "\t\tself.n_iters = n_iters\n",
        "\t\tself.weights = None\n",
        "\t\tself.bias = None\n",
        "\n",
        "\tdef fit(self, X, y):\n",
        "\t\tn_samples, n_features = X.shape\n",
        "\n",
        "\t\t# init parameters\n",
        "\t\tself.weights = np.zeros(n_features)\n",
        "\t\tself.bias = 0\n",
        "\n",
        "        # gradient descent\n",
        "\t\tfor _ in range(self.n_iters):\n",
        "\t\t\ty_predicted = np.dot(X, self.weights) + self.bias\n",
        "            # compute gradients\n",
        "\t\t\tdw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "\t\t\tdb = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # update parameters\n",
        "\t\t\tself.weights -= self.lr * dw\n",
        "\t\t\tself.bias -= self.lr * db\n",
        "\n",
        "\tdef predict(self, X):\n",
        "\t\ty_approximated = np.dot(X, self.weights) + self.bias\n",
        "\t\treturn y_approximated\n",
        "\n",
        "lrg = OurLinearRegression()\n",
        "lrg.fit(X_train, y_train)\n",
        "y_pred = lrg.predict(X_test)\n",
        "for i, e in enumerate(y_pred):\n",
        "\t#print(\"Pred: {}\\tReal: {}\".format(e, y_test[i]))\n",
        "\tpass\n",
        "\n",
        "def accuracy(y_pred, y_real, n_variables):\n",
        "\terr = np.sum(np.square(np.abs(y_pred - y_real))) / (len(y_pred) - n_variables)\n",
        "\terr = 1 / err * 100\n",
        "\treturn err\n",
        "def MAE(y_pred, y_test):\n",
        "\terr = np.sum(np.abs(y_pred-y_test)) / len(y_pred)\n",
        "\treturn err\n",
        "def MSE(y_pred, y_test):\n",
        "\terr = np.sum(np.square(y_pred-y_test)) / len(y_pred)\n",
        "\treturn err\n",
        "print(\"Mean absolute error is {} and square absolute error is {}.\".format(MAE(y_pred, y_test), MSE(y_pred, y_test)))\n",
        "index_list = [i for i in range(len(y_pred))]\n",
        "#print(index_list)\n",
        "plt.scatter(index_list, np.square(y_pred - y_test), c = \"green\")\n",
        "plt.ylim(-2,100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "08QJC1T30FiR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}